# -*- coding: utf-8 -*-
"""Loading Llama 2 from Huggingface.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13agYvd6mcRtptWDVc5FFVzn9bw-MRCci

## **Setting Up the Environment**
"""

!pip install accelerate protobuf sentencepiece torch git+https://github.com/huggingface/transformers huggingface_hub

"""## **Code Summary:**

**accelerate**: Optimizes the performance of models.

**protobuf**: A serialization library used for data exchange.

**sentencepiece**: A tokenizer and detokenizer library.

**torch**: PyTorch, a popular machine learning framework.

**transformers**: The Hugging Face Transformers library.

## **Loading the Pre-trained Language Model (Llama 2)**
"""

from huggingface_hub import login
# Replace 'your-access-token' with your actual Hugging Face access token
login(token="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")

"""Load the pre-trained model and tokenizer"""

model_id = "NousResearch/Llama-2-7b-chat-hf"
#Import the transformers module
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline,  GenerationConfig
import torch

model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map="auto")
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.use_default_system_prompt = False

# Load the summarization pipeline
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Test the summarizer with a longer input text
long_text = (
    "The quick brown fox jumps over the lazy dog. The lazy dog, however, was not really lazy. "
    "It was simply tired from chasing after the quick brown fox all day. The two animals had a "
    "long history of playful rivalry, with the fox always outwitting the dog. Despite their differences, "
    "they shared a bond of mutual respect and friendship."
)

summary = summarizer(long_text, max_length=50, min_length=25, do_sample=False)

print(summary[0]['summary_text'])

# Load the QA pipeline
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

# Define the context and the question
context = (
    "Apple Inc. is an American multinational technology company headquartered in Cupertino, California, "
    "that designs, develops, and sells consumer electronics, computer software, and online services. "
    "It is considered one of the Big Five companies in the U.S. information technology industry, along with "
    "Amazon, Google, Microsoft, and Facebook."
)
question = "Where is Apple Inc. headquartered?"

# Get the answer
answer = qa_pipeline(question=question, context=context)

print(f"Question: {question}")
print(f"Answer: {answer['answer']}")

"""## **Code Summary:**

**AutoModelForCausalLM:** Loads a pre-trained causal language model.

**AutoTokenizer:** Loads the tokenizer associated with the pre-trained model.

**model_id:** Specifies the identifier of the pre-trained model.

**torch_dtype=torch.float16**: Optimizes model performance by using 16-bit floating-point precision.

**device_map="auto":** Automatically selects the appropriate device (CPU or GPU) for model inference.

## **Exploring Model Details**

Get the model configuration
"""

config = model.config
#Retrieves the configuration of the loaded model, which includes details such as the model architecture, number of layers, hidden size, etc.

print(config)

"""# **Model Name and Path:**

**_name_or_path:** “NousResearch/Llama-2-7b-chat-hf”

This tells you the specific model you are using. It’s like knowing the name of the book you’re reading. In this case, it’s the “meta-llama/Llama-2-7b-chat-hf” model from Hugging Face.

# **Hidden Size:**

**hidden_size:** 4096

This indicates the size of the hidden layers in the model. Think of it as the brainpower of the model – a larger hidden size means the model can handle more complex patterns in the data.

# **Number of Hidden Layers:**

**num_hidden_layers:** 32

This tells you how many hidden layers the model has. More layers can enable the model to learn more complex features from the data. In this case, there are 32 layers.
"""

#Outputs a summary of the model architecture showing the various layers and their configurations.
print(model)

"""The print(model) function provides a detailed summary of the model’s architecture. To understand the components, imagine the model as a device that has this simple architecture: Input, Process, Output.

**Input (Embedding)**: Imagine you have a huge dictionary with 32,000 words. Each word is represented by a unique 4096-number code. This is like converting each word into a unique barcode.

**Process (Layers)**: The model processes these barcodes through 32 different steps (layers). At each step, it looks at the words and their contexts, refining its understanding of the input.

**Output (Generating Text)**: After processing, the model uses the refined information to decide which words to generate next, converting the barcodes back into readable words.

## **Generating Text**
"""

# This is the text input you provide to the model. It’s like asking the model a question or giving it a starting sentence.
sample_prompt = "Hello, how are you?"

# The tokenizer converts your text into tokens (numbers that represent words or sub-words). This is necessary because the model works with numbers,
# not raw text. The return_tensors="pt" part tells the tokenizer to return the tokens as a PyTorch tensor,
# which is a data structure used in machine learning.

input_ids = tokenizer.encode(sample_prompt, return_tensors="pt")

# This line checks if you have a GPU available to speed up the processing. Else, it will just use your CPU.
input_ids = input_ids.to('cuda' if torch.cuda.is_available() else 'cpu')

#The model generates a response based on your input tokens:
output = model.generate(input_ids, max_length=50, num_beams=5, no_repeat_ngram_size=2)

# max_length=50:The maximum length of the generated response is 50 tokens. You can of course adjust this to get longer responses.
# num_beams=5: This uses a technique called beam search with 5 beams to generate better quality responses.
# no_repeat_ngram_size=2: This prevents the model from repeating the same phrase or sequence words.
#The tokenizer converts the generated tokens back into human-readable text.

#Decode the output back to text
response = tokenizer.decode(output[0], skip_special_tokens=True)

#Finally, output the respnose
print(f"Generated Response: {response}")

"""## **Code Summary:**

**tokenizer.encode(sample_prompt, return_tensors="pt")**: Encodes the sample prompt into token IDs.

**input_ids.to('cuda' if torch.cuda.is_available() else 'cpu'):** Moves the input IDs to GPU if available.

**model.generate(input_ids, max_length=50, num_beams=5, no_repeat_ngram_size=2):** Generates a response using the model.

**tokenizer.decode(output[0], skip_special_tokens=True):** Decodes the model output into human-readable text.

## **Named Entity Recognition (NER)**
"""

from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline

# Load a pre-trained NER model
model_name = "dbmdz/bert-large-cased-finetuned-conll03-english"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name)

# Create an NER pipeline
ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer)

# Test the NER pipeline with a sample sentence
test_sentence = "Apple is planning to build a new campus in Austin."
result = ner_pipeline(test_sentence)
print(result)

"""This code initializes the NER pipeline with the pre-trained BERT model fine-tuned on the CoNLL-2003 dataset for NER tasks. It then runs a test input to extract named entities from the sentence.

Pay attention to the last section of the output. There will be a list of detected entities in the text along with their labels and confidence scores.

“Apple” is detected as “I-ORG” with a confidence level of “0.99”.
"""

def display_masked_sentence(sentence, ner_results):
    masked_sentence = sentence
    for entity in ner_results:
        entity_word = entity['word']
        entity_label = entity['entity']
        masked_sentence = masked_sentence.replace(entity_word, f"[{entity_label}]")
    return masked_sentence

# Test the function with the NER results
masked_sentence = display_masked_sentence(test_sentence, result)
print(masked_sentence)



"""This function takes the original sentence and the NER results as input. It loops through the detected entities and replaces each entity in the sentence with its corresponding label (e.g., [I-ORG], [I-LOC])."""

# Load the summarization pipeline
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# Test the summarizer with a longer input text
long_text = (
    "The quick brown fox jumps over the lazy dog. The lazy dog, however, was not really lazy. "
    "It was simply tired from chasing after the quick brown fox all day. The two animals had a "
    "long history of playful rivalry, with the fox always outwitting the dog. Despite their differences, "
    "they shared a bond of mutual respect and friendship."
)

summary = summarizer(long_text, max_length=50, min_length=25, do_sample=False)

print(summary[0]['summary_text'])

# Load the QA pipeline
qa_pipeline = pipeline("question-answering", model="deepset/roberta-base-squad2")

# Define the context and the question
context = (
    "Apple Inc. is an American multinational technology company headquartered in Cupertino, California, "
    "that designs, develops, and sells consumer electronics, computer software, and online services. "
    "It is considered one of the Big Five companies in the U.S. information technology industry, along with "
    "Amazon, Google, Microsoft, and Facebook."
)
question = "Where is Apple Inc. headquartered?"

# Get the answer
answer = qa_pipeline(question=question, context=context)

print(f"Question: {question}")
print(f"Answer: {answer['answer']}")